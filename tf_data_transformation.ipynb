{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        }
      },
      "source": "### Dataset支持特殊操作:Transformation \n ####  1.一個Dataset可以透過Transformation變成另一個Dataset\n #### 2.通過transformation來做數據變化或pre-processing(預處理)\n #### 3.常用map() flat_map(),filter(),shuffle(),repeat()\n"
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.map()\n#### map可以將map_func函數映射到數據集\n\n#### 函數形式：flat_map(map_func，num_parallel_calls\u003dNone)\n#### 參數map_func:映射函數\n#### 參數num_parallel_calls：表示要並行處理的數字元素。如果未指定，將按順序處理元素。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import tensorflow as tf\nimport numpy as np\n\na1 \u003d np.array([1.0,2.0,3.0,4.0,5.0])\ndataset \u003d tf.data.Dataset.from_tensor_slices(a1)\ndataset \u003d dataset.map(lambda x: x*2) \niterator \u003d dataset.make_one_shot_iterator()\n\none_element \u003d iterator.get_next()\nwith tf.Session() as sess:\n    for i in range(len(a1)):\n        print(sess.run(one_element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.flat_map()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### 與map不同是傳入數據必須是一個dataset object\n#### 函數形式：flat_map(map_func)\n#### 參數map_func: 映射函數",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import tensorflow as tf\nimport numpy as np\n\na2 \u003d np.array([1.0,2.0,3.0,4.0,5.0])\ndataset2 \u003d tf.data.Dataset.from_tensor_slices(a2)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### 若傳入函數不是一個object物件會報錯",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-4-7d1232286727\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#映射的函數--\u003e直接從元素操作\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mdataset2\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mD:\\Anaconda\\envs\\MLlearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1068\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[0;32m-\u003e 1070\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m   def interleave(self,\n",
            "\u001b[0;32mD:\\Anaconda\\envs\\MLlearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         experimental_nested_dataset_support\u003dTrue)\n\u001b[1;32m   2678\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NestedDatasetComponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2679\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`map_func` must return a `Dataset` object.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_classes\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_types\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: `map_func` must return a `Dataset` object."
          ],
          "ename": "TypeError",
          "evalue": "`map_func` must return a `Dataset` object.",
          "output_type": "error"
        }
      ],
      "source": "dataset2 \u003d dataset2.flat_map(lambda x: x+1)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### 將函數包裝為Dataset object",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": "#映射的函數--\u003e要包裝成Dataset\ndataset2 \u003d dataset2.flat_map(lambda x:tf.data.Dataset.from_tensor_slices(x+[1]))\n\niterator2 \u003d dataset2.make_one_shot_iterator()\none_element1 \u003d iterator2.get_next()\nwith tf.Session() as sess:\n    for i in range(len(a2)):\n        print(sess.run(one_element1))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.zip()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import tensorflow as tf\nimport numpy as np\n\n#將給定的dataet壓縮再一起\ndataset3 \u003d tf.data.Dataset.from_tensor_slices(np.array([2.0,2.0,2.0]))\ndataset4 \u003d tf.data.Dataset.from_tensor_slices(np.array([3.0,3.0,3.0]))\ndataset_zip \u003d tf.data.Dataset.zip((dataset3,dataset4))\niterator3\u003d dataset_zip.make_one_shot_iterator()\none_element2 \u003d iterator3.get_next()\n\nwith tf.Session() as sess:\n    for i in range(3): #顯示出0~2的元素\n        print(sess.run(one_element2))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.concatenate()\n#### 將兩個dataset進行串聯(合併)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\n\ndataset_1 \u003d tf.data.Dataset.from_tensor_slices([1.0,2.0,3.0])\ndataset_2 \u003d tf.data.Dataset.from_tensor_slices([4.0,5.0,6.0])\n\ndataset_concat \u003d dataset_1.concatenate(dataset_2)\n\niterator \u003d dataset_concat.make_one_shot_iterator()\none_element \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n    for i in range(6):\n     print(sess.run(one_element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.filter()\n#### 對傳入的Dataset進行條件過濾",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "5.0\n6.0\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\n\ndataset \u003d tf.data.Dataset.from_tensor_slices([1.0,2.0,3.0,4.0,5.0,6.0])\ndataset \u003d dataset.filter(lambda x: x\u003e4)\n\niterator \u003d dataset.make_one_shot_iterator()\none_element \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n    for i in range(2):\n     print(sess.run(one_element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.generator()\n#### 創建dataset, dataset元素由generator來生成",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "0\n1\n2\n3\n4\n5\n6\n7\n8\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\n#定義一個generator\ndef data_generator():\n    dataset \u003d np.array(range(9))\n    for i in dataset:\n        yield i\ndataset \u003d tf.data.Dataset.from_generator(data_generator,tf.int32)\niterator \u003d dataset.make_one_shot_iterator()\none_element \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n    for i in range(9):\n     print(sess.run(one_element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.barch()\n#### 將數據集的連續元素堆疊成單個元素。\n\n#### 函數形式：batch(batch_size,drop_remainder\u003dFalse)\n\n#### 參數batch_size:表示要在單個批次中合併的此數據集的連續元素個數。\n#### 參數drop_remainder：表示在少於batch_size元素的情況下是否應刪除最後一批;默認是不刪除。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "(array([0, 1, 2, 3], dtype\u003dint64), array([ 0, -1, -2, -3], dtype\u003dint64))\n(array([4, 5, 6, 7], dtype\u003dint64), array([-4, -5, -6, -7], dtype\u003dint64))\n(array([ 8,  9, 10, 11], dtype\u003dint64), array([ -8,  -9, -10, -11], dtype\u003dint64))\n(array([12, 13, 14, 15], dtype\u003dint64), array([-12, -13, -14, -15], dtype\u003dint64))\n(array([16, 17, 18, 19], dtype\u003dint64), array([-16, -17, -18, -19], dtype\u003dint64))\n(array([20, 21, 22, 23], dtype\u003dint64), array([-20, -21, -22, -23], dtype\u003dint64))\n(array([24, 25, 26, 27], dtype\u003dint64), array([-24, -25, -26, -27], dtype\u003dint64))\n(array([28, 29, 30, 31], dtype\u003dint64), array([-28, -29, -30, -31], dtype\u003dint64))\n(array([32, 33, 34, 35], dtype\u003dint64), array([-32, -33, -34, -35], dtype\u003dint64))\n(array([36, 37, 38, 39], dtype\u003dint64), array([-36, -37, -38, -39], dtype\u003dint64))\n(array([40, 41, 42, 43], dtype\u003dint64), array([-40, -41, -42, -43], dtype\u003dint64))\n(array([44, 45, 46, 47], dtype\u003dint64), array([-44, -45, -46, -47], dtype\u003dint64))\n(array([48, 49, 50, 51], dtype\u003dint64), array([-48, -49, -50, -51], dtype\u003dint64))\n(array([52, 53, 54, 55], dtype\u003dint64), array([-52, -53, -54, -55], dtype\u003dint64))\n(array([56, 57, 58, 59], dtype\u003dint64), array([-56, -57, -58, -59], dtype\u003dint64))\n(array([60, 61, 62, 63], dtype\u003dint64), array([-60, -61, -62, -63], dtype\u003dint64))\n(array([64, 65, 66, 67], dtype\u003dint64), array([-64, -65, -66, -67], dtype\u003dint64))\n(array([68, 69, 70, 71], dtype\u003dint64), array([-68, -69, -70, -71], dtype\u003dint64))\n(array([72, 73, 74, 75], dtype\u003dint64), array([-72, -73, -74, -75], dtype\u003dint64))\n(array([76, 77, 78, 79], dtype\u003dint64), array([-76, -77, -78, -79], dtype\u003dint64))\n(array([80, 81, 82, 83], dtype\u003dint64), array([-80, -81, -82, -83], dtype\u003dint64))\n(array([84, 85, 86, 87], dtype\u003dint64), array([-84, -85, -86, -87], dtype\u003dint64))\n(array([88, 89, 90, 91], dtype\u003dint64), array([-88, -89, -90, -91], dtype\u003dint64))\n(array([92, 93, 94, 95], dtype\u003dint64), array([-92, -93, -94, -95], dtype\u003dint64))\n(array([96, 97, 98, 99], dtype\u003dint64), array([-96, -97, -98, -99], dtype\u003dint64))\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\n\n\"\"\"\ndataset.range(start,stop,step) \n#len(args\u003d1)--\u003edataset.range(stop)  若無宣告:默認start\u003d0 step\u003d1\n#len(args\u003d2)--\u003edataset.range(start,stop) \n#len(args\u003d3)--\u003edataset.range(start,stop,step)\n\"\"\"\ndataset1 \u003d tf.data.Dataset.range(100)\ndataset2 \u003d tf.data.Dataset.range(0,-100,-1)\ndataset \u003d tf.data.Dataset.zip((dataset1,dataset2))\nbatched_dataset \u003d dataset.batch(4)\n\niterator \u003d batched_dataset.make_one_shot_iterator()\none_element \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n    for i in range(25):\n     print(sess.run(one_element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# padded_batch\n#### 在batch的例子中,tensors大小是相同的,然後很多工作輸入數據大小並不一,為了解決這種情況,dataset.padded_batch()能將不同size的tensor進行batch,需要指定進行pad的維度\n#### 將數據集的連續元素組合到填充批次中,此轉換將輸入數據集的多個連續元素組合為單個元素\n#### 函數形式：padded_batch(batch_size,padded_shapes,padding_values\u003dNone,drop_remainder\u003dFalse)\n\n#### 參數batch_size：表示要在單個批次中合併的此數據集的連續元素數。\n#### 參數padded_shapes：嵌套結構tf.TensorShape或tf.int64類似矢量張量的對象，表示batch之前應將每個輸入元素填充成相應的維度形狀。任何未知的維度（例如，tf.Dimension(None)在一個tf.TensorShape或-1類似張量的物體中）將被填充到每個batch中元素最大的維度尺寸。\n#### 參數padding_values:(可選）標量形狀的嵌套結構tf.Tensor，表示用於各個組件的填充值。默認值0用於數字類型，空字符串用於字符串類型。\n#### 參數drop_remainder:(可選）一個tf.bool標量tf.Tensor，表示在少於batch_size元素的情況下是否應刪除最後一批;默認行為是不刪除較小的批處理。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[[0 0 0]\n [1 0 0]\n [2 2 0]\n [3 3 3]]\n[[4 4 4 4 0 0 0]\n [5 5 5 5 5 0 0]\n [6 6 6 6 6 6 0]\n [7 7 7 7 7 7 7]]\n[[ 8  8  8  8  8  8  8  8  0  0  0]\n [ 9  9  9  9  9  9  9  9  9  0  0]\n [10 10 10 10 10 10 10 10 10 10  0]\n [11 11 11 11 11 11 11 11 11 11 11]]\n[[12 12 12 12 12 12 12 12 12 12 12 12  0  0  0]\n [13 13 13 13 13 13 13 13 13 13 13 13 13  0  0]\n [14 14 14 14 14 14 14 14 14 14 14 14 14 14  0]\n [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]]\n[[16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16  0  0  0]\n [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17  0  0]\n [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18  0]\n [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19]]\n[[20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20  0  0  0]\n [21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21  0  0]\n [22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22  0]\n [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23]]\n[[24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n   0  0  0]\n [25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25\n  25  0  0]\n [26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n  26 26  0]\n [27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27\n  27 27 27]]\n[[28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n  28 28 28 28  0  0  0]\n [29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29\n  29 29 29 29 29  0  0]\n [30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30\n  30 30 30 30 30 30  0]\n [31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n  31 31 31 31 31 31 31]]\n[[32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32\n  32 32 32 32 32 32 32 32  0  0  0]\n [33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n  33 33 33 33 33 33 33 33 33  0  0]\n [34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34\n  34 34 34 34 34 34 34 34 34 34  0]\n [35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n  35 35 35 35 35 35 35 35 35 35 35]]\n[[36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n  36 36 36 36 36 36 36 36 36 36 36 36  0  0  0]\n [37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37 37\n  37 37 37 37 37 37 37 37 37 37 37 37 37  0  0]\n [38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n  38 38 38 38 38 38 38 38 38 38 38 38 38 38  0]\n [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n  39 39 39 39 39 39 39 39 39 39 39 39 39 39 39]]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\"\"\"\n註1:tf.cast(x, dtype, name\u003dNone)\n此函数是类型转换函数\n\n参数\nx：imput\ndtype：轉換目標類型\nname：名稱\nex.\ntensor `a` is [1.8, 2.2], dtype\u003dtf.float\ntf.cast(a, tf.int32) \u003d\u003d\u003e [1, 2]  # dtype\u003dtf.int32\n\n註2:tf.fill（dims,value,name \u003d None）\n創建一個維度為dims,值為value的tensor對象．該操作會創建一個維度為dims的tensor對象，並將其值設置為value，該tensor對像中的值類型和value一致\n\n當value為０時，該方法等同於tf.zeros()\n當value為１時，該方法等同於tf.ones()\n參數:\ndims: 類型為int32的tensor對象，用於表示輸出的維度(1-D, nD)，通常為一個int32數組，如：[1], [2,3]等\nvalue: 常量值(字符串，數字等)，該參數用於設置到最終返回的tensor對象值中\nname: 當前操作別名(可選)\n\"\"\"\nimport tensorflow as tf\nimport numpy as np\n\ndataset \u003d tf.data.Dataset.range(100)\n#製造不同size的tensor\ndataset \u003d dataset.map(lambda x: tf.fill([tf.cast(x,tf.int32)],x)) #註1:tf.cast() 註2:tf.fill()\ndataset \u003d dataset.padded_batch(4,padded_shapes\u003d[None])\niterator \u003d dataset.make_one_shot_iterator()\none_element \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n    for i in range(10):\n     print(sess.run(one_element))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.repeat()\n#### 重複此數據集count次數\n\n#### 函數形式：repeat(count\u003dNone)\n\n#### 參數count:(可選）表示數據集應重複的次數。默認行為（如果count是None或-1）是無限期重複的數據集。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\n\ndataset \u003d tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6])\ndataset \u003d dataset.repeat(3)\niterator \u003d dataset.make_one_shot_iterator()\none_element \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n    for i in range(18):\n     print(sess.run(one_element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.shuffle() \n#### 函數形式：shuffle(buffer_size,seed\u003dNone,reshuffle_each_iteration\u003dNone)\n#### 參數buffer_size:表示新數據集將從中採樣的數據集中的元素數。\n#### 參數seed:(可選）表示將用於創建分佈的隨機種子。\n#### 參數reshuffle_each_iteration:(可選）一個布爾值，如果為true，則表示每次迭代時都應對數據集進行偽隨機重組。（默認為True。）\n#### https://www.twblogs.net/a/5c0e196cbd9eee5e41834221 有關buffer size詳解",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[1]\n[3]\n[2]\n[4]\n[6]\n[7]\n[8]\n[5]\n[9]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\n\ndataset \u003d tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9])\n\ndataset\u003ddataset.shuffle(buffer_size\u003d 2)\ndataset \u003d dataset.batch(batch_size\u003d 1)\n\niterator \u003d dataset.make_one_shot_iterator()\n\nelement \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n   for i in range(9):\n       \n       print(sess.run(element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.skip()\n#### 生成一個跳過count元素的數據集。\n\n#### 函數形式：skip(count)\n\n#### 參數count:表示應跳過以形成新數據集的此數據集的元素數。如果count大於此數據集的大小，則新數據集將不包含任何元素。如果count 為-1，則跳過整個數據集。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "5\n6\n7\n8\n9\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\ndataset \u003d tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9])\n\n#跳過前4個\ndataset\u003ddataset.skip(4)\n\niterator \u003d dataset.make_one_shot_iterator()\n\nelement \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n   for i in range(5):\n       print(sess.run(element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# dataset.take()\n#### 提取前count個元素形成性數據集\n\n#### 函數形式：take(count)\n\n#### 參數count:表示應該用於形成新數據集的此數據集的元素數。如果count為-1，或者count大於此數據集的大小，則新數據集將包含此數據集的所有元素。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1\n2\n2\n3\n4\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\ndataset \u003d tf.data.Dataset.from_tensor_slices([1,2,2,3,4,5,6,7,8,9])\n\n#提取前5个元素形成新数据\ndataset\u003ddataset.take(5)\n\niterator \u003d dataset.make_one_shot_iterator()\n\nelement \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n   for i in range(5):\n       print(sess.run(element))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}