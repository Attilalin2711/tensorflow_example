{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "#### iterator與dataset關係:https://www.twblogs.net/a/5b8e85d62b7177188345d355",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "## Dataset和Iterator關係\n#### 如果Dataset是一個水池的話,數據就如同水池中的水,Iterator可以當作一個水管,在tensorflow裡,正式通過Iteraotr這根水管,源源不斷的從Dataset中取出數據,為了應付多變的環境,水管也需要變化,Iterator也有許多種類"
    },
    {
      "cell_type": "markdown",
      "source": "#### 構建了表示輸入數據的Dataset後，下一步就是創建Iterator來訪問該數據集中的元素。tf.data API目前支持下列迭代器，其複雜程度逐漸上升：\n\n#### （1）單次迭代器(一次性水管)\n#### （2）可初始化迭代器(訂製水管)\n#### （3）可重新初始化迭代器(能接不同水池的水管)\n#### （4）可feeding迭代器(水管的轉換器)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# （1）單次迭代器(一次性水管)\n#### 單次迭代器是最簡單的迭代器形式，僅支持對數據集進行一次迭代，不需要顯式初始化。單次迭代器可以處理現有的基於隊列的輸入管道支持的幾乎所有情況，但不支持參數化。以Dataset.range()為例：",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "0\n1\n2\n3\n4",
            "\n"
          ],
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "\u0027上面的代碼非常簡單，首先創建了一個包含 0 到 4 的數據集。然後，創建了一個單次迭代器。\\n\\n通過循環調用 get_next() 方法就可以將數據取出。\\n\\n需要注意的是，通常用 try-catch 配合使用，當 Dataset 中的數據被讀取完畢的時候，程序會拋出異常，獲取這個異常就可以從容結束本次數據的迭代。\\n\\n然後， iterator 就完成了它的歷史使命。單次的迭代器，不支持動態的數據集，它比較單純，它不支持參數化。\\n\\n什麼是參數化呢？你可以理解爲單次的 Iterator 認死理，它需要 Dataset 在程序運行之前就確認自己的大小，但我們都知道 Tensorflow 中有一種 feeding 機制，它允許我們在程序運行時再真正決定我們需要的數據，很遺憾，單次的 Iterator 不能滿足這要的要求。\u0027"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 1
        }
      ],
      "source": "import tensorflow as tf\n\ndataset \u003d tf.data.Dataset.range(5)\niterator \u003d dataset.make_one_shot_iterator() ##創建單次迭代器\n\nwith tf.Session() as sess:\n    while True:\n        try:\n            print(sess.run(iterator.get_next()))\n        except tf.errors.OutOfRangeError:\n            break",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### 上面的代碼非常簡單，首先創建了一個包含 0 到 4 的數據集。然後，創建了一個單次迭代器。\n#### 通過循環調用 get_next() 方法就可以將數據取出。\n#### 需要注意的是，通常用 try-catch 配合使用，當 Dataset 中的數據被讀取完畢的時候，程序會拋出異常，獲取這個異常就可以從容結束本次數據的迭代。\n#### 然後， iterator 就完成了它的歷史使命。單次的迭代器，不支持動態的數據集，它比較單純，它不支持參數化。\n##什麼是參數化呢？你可以理解爲單次的 Iterator 認死理，它需要 Dataset 在程序運行之前就確認自己的大小，但我們都知道 Tensorflow 中有一種 feeding 機制，它允許我們在程序運行時再真正決定我們需要的數據，很遺憾，單次的 Iterator 不能滿足這要的要求。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 參數化,可以看見會報錯\n##單次 Iterator 無法滿足參數化的要求，但有其他類型的 Iterator 可以完成這個目標。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-2-5f9e35d91829\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 5\u001b[0;31m \u001b[0miterator\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mD:\\Anaconda\\envs\\MLlearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmake_one_shot_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \"(Original error: %s)\" % err)\n\u001b[1;32m    204\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 205\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     return iterator_ops.Iterator(\n",
            "\u001b[0;32mD:\\Anaconda\\envs\\MLlearning\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot capture a placeholder (name:Placeholder, type:Placeholder) by value."
          ],
          "ename": "ValueError",
          "evalue": "Cannot capture a placeholder (name:Placeholder, type:Placeholder) by value.",
          "output_type": "error"
        }
      ],
      "source": "import tensorflow as tf\n\nmax_value \u003d tf.placeholder(tf.int64,shape\u003d[])\ndataset \u003d tf.data.Dataset.range(max_value)\niterator \u003d dataset.make_one_shot_iterator()\n\nwith tf.Session() as sess:\n\n    while True:\n            try:\n                print(sess.run(iterator.get_next(),feed_dict\u003d{max_value:5}))\n            except tf.errors.OutOfRangeError:\n                break",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n# (2）可初始化迭代器\n### 單次 Iterator 無法滿足參數化的要求，但有其他類型的 Iterator 可以完成這個目標。\n### 需先透過iterator.initializer初始化iterator，才能使用可初始化迭代器\n#### 雖然有些不便，但它允許您使用一個或多個tf.placeholder()張量（可在初始化迭代器時饋送）參數化數據集的定義。繼續以Dataset.range()為例：\n#### 上面的程式碼可以改寫為",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\n\nmax_value \u003d tf.placeholder(tf.int64, shape\u003d[])\ndataset \u003d tf.data.Dataset.range(max_value)\niterator \u003d dataset.make_initializable_iterator() #創建可初始化迭代器\n\nwith tf.Session() as sess:\n    #initialize an iterator over a dataset with 10 element\n    sess.run(iterator.initializer, feed_dict\u003d{max_value: 5})\n    while True:\n        try:\n            print(sess.run(iterator.get_next()))\n        except tf.errors.OutOfRangeError:\n            break\n    #initialize the same iterator over a dataset with 100 element\n    sess.run(iterator.initializer, feed_dict\u003d{max_value: 6})\n    while True:\n        try:\n            print(sess.run(iterator.get_next()))\n        except tf.errors.OutOfRangeError:\n            break",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### output: 01234,012345 \n\n#### 跟單次 Iterator 的代碼只有 2 處不同。\n\n#### 1、創建的方式不同，iterator.make_initialnizer()。\n\n#### 2、每次重新初始化的時候，都要調用sess.run(iterator.initializer)\n\n#### 你可以這樣理解，Dataset 這個水池連續裝了 2 次水，每次水量不一樣，但可初始化的 Iterator 很好地處理了這件事情，但需要注意的是，這個時候 Iterator 還是面對同一個 Dataset。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# reinitializable iterator(能夠接不同水池的水管)\n#### 有時候，需要一個 Iterator 從不同的 Dataset 對象中讀取數值。Tensorflow 針對這種情況，提供了一個可以重新初始化的 Iterator，它的用法相對而言，比較複雜，但好在不是很難理解。\n### 可以通過不同的Dataset進行initialize\n#### ex.有一個input,並對其進行隨機擾動泛化數據,並有一個驗證input,評估未修改數據的預測\n#### 上述的例子中常會使用不同的Dataset,這些Dataset有相同結構",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "3\n-8\n6\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n0\n1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n5\n8\n-4\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n0\n1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n-8\n6\n2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n0\n1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\ntraining_dataset \u003d tf.data.Dataset.range(10).map(\n    lambda x: x+tf.random.uniform([],-10,10,tf.int64))\nvalidation_dataset \u003d tf.data.Dataset.range(5)\n\n\"\"\" 我們可以使用training_dataset或是\nvalidation_dataset的output.types與output.value來定義\nreinitializable iterator的結構,兩個是可以相互兼容\"\"\"\n\niterator \u003d tf.data.Iterator.from_structure(training_dataset.output_types,\n                                           training_dataset.output_shapes)\n\ntraining_init_op \u003d iterator.make_initializer(training_dataset)\nvalidation_init_op \u003d iterator.make_initializer(validation_dataset)\n\nnext_element \u003d iterator.get_next()\n\nwith tf.Session() as sess:\n    for _ in range(3): #Run 3 epochs\n        #基於training dataset初始化iterator\n        sess.run(training_init_op)\n        for _ in range(3):\n            print(sess.run(next_element))\n        print(\u0027\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u0027)\n        #基於validation dataset初始化iterator\n        sess.run(validation_init_op)\n        for _ in range(2):\n            print(sess.run(next_element))\n        print(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 核心代碼\n##### iterator \u003d tf.data.Iterator.from_structure(training_data.output_types,training_data.output_shapes)\n\n##### train_op \u003d iterator.make_initializer(training_data)\n##### validation_op \u003d iterator.make_initializer(validation_data)\n#### Iterator 可以接多個水池裏面的水，但是要求這水池裏面的水是同樣的品質。也就是，多個 Dataset 中它們的元素數據類型和形狀應該是一致的。通過 from_structure() 統一規格，後面的 2 句代碼可以看成是 2 個水龍頭，它們決定了放哪個水池當中的水。\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### 不知道大家注意到一點沒有？每次 Iterator 切換時，數據都從頭開始打印了。如果，不想這種情況發生，就需要接下來介紹的另外一種 Iterator。",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# feedable iterator(水管的轉換器)\n#### Tensorflow 最美妙的一個地方就是 feeding 機制，它決定了很多東西可以在程序運行時，動態填充，這其中也包括了 Iterator。\n#### 不同的 Dataset 用不同的 Iterator，然後利用 feeding 機制，動態決定，聽起來就很棒，不是嗎？\n\n#### 無論是在機器學習還是深度學習當中，訓練集、驗證集、測試集是大家繞不開的話題，但偏偏它們要分離開來，偏偏它們的數據類型又一致，所以，經常我們要寫同樣的重複的代碼。\n\n#### 複用，是軟件開發中一個重要的思想。\n\n#### 可饋送的 Iterator 一定程度上可以解決重複的代碼，同時又將訓練集和驗證集的操作清晰得分離開來。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1\n-6\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n0\n1\n2\n3\n4\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n7\n-2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n0\n1\n2\n3\n4\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n2\n2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n0\n1\n2\n3\n4\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\nimport numpy as np\n\ntraining_dataset \u003d tf.data.Dataset.range(100).map(\n    lambda x: x+tf.random.uniform([], -10, 10, tf.int64))\ntesting_dataset \u003d tf.data.Dataset.range(50)\n\"\"\"feedable iterator是藉由handle placeholder與其structure所定義\n   由於training與testing 兩個dataset有相同sturcture,可使用其output性質初始iterator\"\"\"\nhandle \u003d tf.placeholder(tf.string, shape \u003d [])\niterator \u003d tf.data.Iterator.from_string_handle(\n    handle,training_dataset.output_types,training_dataset.output_shapes\n)\nnext_element \u003d iterator.get_next()\n#可使用不同類型的iterator來填充 feedable_iterator\ntraining_iterator \u003d  training_dataset.make_one_shot_iterator()\ntesting_iterator \u003d testing_dataset.make_initializable_iterator()\n\nwith tf.Session() as sess:\n    #iterator.string_handle()可返回一個tensor來賦值或填充handle這個placeholder\n    training_handle \u003d sess.run(training_iterator.string_handle())\n    testing_handle \u003d sess.run(testing_iterator.string_handle())\n    for _ in range(3):\n    \n        for _ in range(2):\n            print(sess.run(next_element, feed_dict\u003d{handle: training_handle}))\n        print(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n        \n        sess.run(testing_iterator.initializer)\n        \n        for _ in range(5):\n            print(sess.run(next_element,feed_dict\u003d{handle: testing_handle}))\n        \n        print(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 看起來跟前面以小節的代碼沒有多大區別。核心代碼如下：\n####  handle \u003d tf.placeholder(tf.string,shape\u003d[])\n#### iterator \u003d tf.data.Iterator.from_string_handle(\n#### handle,train_data.output_types,train_data.output_shapes)\n#### train_iterator_handle \u003d sess.run(train_op.string_handle())\n#### val_iterator_handle \u003d sess.run(validation_op.string_handle())\n#### 它是通過一個 string 類型的 handle 實現的。需要注意的一點是，string_handle() 方法返回的是一個 Tensor，只有運行一個 Tensor 纔會返回 string 類型的 handle。不然，程序會報錯。\n\n####如果用圖表的形式加深理解的話，那就是可饋送 Iterator 的方式，可以自主決定用哪個 Iterator，就好比不同的水池有不同的水管，不需要用同一根水管接到不同的水池當中去了。\n\n#### 可饋送的 Iterator 和可重新初始化的 Iterator 非常相似，但是，可饋送的 Iterator 在不同的 Iterator 切換的時候，可以做到不從頭開始。\n\n# 總結\n\n#### 1、 單次 Iterator ，它最簡單，但無法重用，無法處理數據集參數化的要求。 \n#### 2、 可以初始化的 Iterator ，它可以滿足 Dataset 重複加載數據，滿足了參數化要求。 \n#### 3、可重新初始化的 Iterator，它可以對接不同的 Dataset，也就是可以從不同的 Dataset 中讀取數據。 \n#### 4、可饋送的 Iterator，它可以通過 feeding 的方式，讓程序在運行時候選擇正確的 Iterator,它和可重新初始化的 Iterator 不同的地方就是它的數據在不同的 Iterator 切換時，可以做到不重頭開始讀取數據。\n\n#### 終上所述，在真實的神經網絡訓練過程當中，可饋送的 Iterator 是最值得推薦的方式。",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}